{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset with Tensorflow API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "\n",
    "import struct\n",
    " \n",
    "def load_mnist(path, kind='train'):\n",
    "    \n",
    "    if kind == 'train' : \n",
    "        \n",
    "        labels_path = 'train-labels.idx1-ubyte'\n",
    "        images_path = 'train-images.idx3-ubyte'\n",
    "    else : \n",
    "        labels_path = 't10k-labels.idx1-ubyte'\n",
    "        images_path = 't10k-images.idx3-ubyte'\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', \n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, \n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", \n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, \n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    "        images = ((images / 255.) - .5) * 2\n",
    " \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000,  Columns: 784\n",
      "Rows: 10000,  Columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_mnist('./', kind='train')\n",
    "print('Rows: %d,  Columns: %d' %(X_train.shape[0], \n",
    "                                 X_train.shape[1]))\n",
    "X_test, y_test = load_mnist('./', kind='t10k')\n",
    "print('Rows: %d,  Columns: %d' %(X_test.shape[0], \n",
    "                                 X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vals = np.mean(X_train,axis=0)\n",
    "\n",
    "std_dev = np.std(X_train)\n",
    "\n",
    "X_train_centered = (X_train-mean_vals)/std_dev\n",
    "\n",
    "X_test_centered = (X_test-mean_vals)/std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train_centered.shape[1]\n",
    "\n",
    "n_classes = 10 \n",
    "\n",
    "random_seed = 123 \n",
    "\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with g.as_default() : \n",
    "    \n",
    "    tf.set_random_seed(random_seed)\n",
    "    \n",
    "    tf_x = tf.placeholder(shape=(None,n_features),dtype=tf.float32,name=\"tf_x\")\n",
    "    tf_y = tf.placeholder(shape=None,dtype=tf.int32,name=\"tf_y\")\n",
    "    \n",
    "    y_onehot = tf.one_hot(indices=tf_y,depth=n_classes)\n",
    "    \n",
    "    h1 = tf.layers.dense(inputs=tf_x,activation=tf.tanh,units=50,name=\"layers\")\n",
    "    \n",
    "    h2 = tf.layers.dense(inputs=h1,activation=tf.tanh,name=\"layer2\",units=50)\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=h2,units=10,activation=None,name=\"output_layer\")\n",
    "    \n",
    "    predictions = {'classes':tf.argmax(logits,axis=1,name=\"predicted_classes\"),\n",
    "                    'probs':tf.nn.softmax(logits,name=\"softmax_tensor\")\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with g.as_default() : \n",
    "    \n",
    "    cost = tf.losses.softmax_cross_entropy(onehot_labels=y_onehot,logits=logits)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    \n",
    "    train_op = optimizer.minimize(loss=cost)\n",
    "    \n",
    "    init_op = tf.global_variables_initializer()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_generator(X, y, batch_size=128, shuffle=False):\n",
    "    X_copy = np.array(X)\n",
    "    y_copy = np.array(y)\n",
    "    \n",
    "    if shuffle:\n",
    "        data = np.column_stack((X_copy, y_copy))\n",
    "        np.random.shuffle(data)\n",
    "        X_copy = data[:, :-1]\n",
    "        y_copy = data[:, -1].astype(int)\n",
    "    \n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield (X_copy[i:i+batch_size, :], y_copy[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(graph=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Epoch  1  Avg. Training Loss: 1.5573\n",
      " -- Epoch  2  Avg. Training Loss: 1.2532\n",
      " -- Epoch  3  Avg. Training Loss: 1.0854\n",
      " -- Epoch  4  Avg. Training Loss: 0.9738\n",
      " -- Epoch  5  Avg. Training Loss: 0.8924\n",
      " -- Epoch  6  Avg. Training Loss: 0.8296\n",
      " -- Epoch  7  Avg. Training Loss: 0.7794\n",
      " -- Epoch  8  Avg. Training Loss: 0.7381\n",
      " -- Epoch  9  Avg. Training Loss: 0.7032\n",
      " -- Epoch 10  Avg. Training Loss: 0.6734\n",
      " -- Epoch 11  Avg. Training Loss: 0.6475\n",
      " -- Epoch 12  Avg. Training Loss: 0.6247\n",
      " -- Epoch 13  Avg. Training Loss: 0.6045\n",
      " -- Epoch 14  Avg. Training Loss: 0.5864\n",
      " -- Epoch 15  Avg. Training Loss: 0.5700\n",
      " -- Epoch 16  Avg. Training Loss: 0.5551\n",
      " -- Epoch 17  Avg. Training Loss: 0.5415\n",
      " -- Epoch 18  Avg. Training Loss: 0.5290\n",
      " -- Epoch 19  Avg. Training Loss: 0.5175\n",
      " -- Epoch 20  Avg. Training Loss: 0.5068\n",
      " -- Epoch 21  Avg. Training Loss: 0.4968\n",
      " -- Epoch 22  Avg. Training Loss: 0.4875\n",
      " -- Epoch 23  Avg. Training Loss: 0.4788\n",
      " -- Epoch 24  Avg. Training Loss: 0.4706\n",
      " -- Epoch 25  Avg. Training Loss: 0.4629\n",
      " -- Epoch 26  Avg. Training Loss: 0.4556\n",
      " -- Epoch 27  Avg. Training Loss: 0.4487\n",
      " -- Epoch 28  Avg. Training Loss: 0.4422\n",
      " -- Epoch 29  Avg. Training Loss: 0.4359\n",
      " -- Epoch 30  Avg. Training Loss: 0.4300\n",
      " -- Epoch 31  Avg. Training Loss: 0.4243\n",
      " -- Epoch 32  Avg. Training Loss: 0.4189\n",
      " -- Epoch 33  Avg. Training Loss: 0.4138\n",
      " -- Epoch 34  Avg. Training Loss: 0.4088\n",
      " -- Epoch 35  Avg. Training Loss: 0.4041\n",
      " -- Epoch 36  Avg. Training Loss: 0.3995\n",
      " -- Epoch 37  Avg. Training Loss: 0.3951\n",
      " -- Epoch 38  Avg. Training Loss: 0.3909\n",
      " -- Epoch 39  Avg. Training Loss: 0.3868\n",
      " -- Epoch 40  Avg. Training Loss: 0.3829\n",
      " -- Epoch 41  Avg. Training Loss: 0.3791\n",
      " -- Epoch 42  Avg. Training Loss: 0.3754\n",
      " -- Epoch 43  Avg. Training Loss: 0.3718\n",
      " -- Epoch 44  Avg. Training Loss: 0.3684\n",
      " -- Epoch 45  Avg. Training Loss: 0.3651\n",
      " -- Epoch 46  Avg. Training Loss: 0.3618\n",
      " -- Epoch 47  Avg. Training Loss: 0.3587\n",
      " -- Epoch 48  Avg. Training Loss: 0.3556\n",
      " -- Epoch 49  Avg. Training Loss: 0.3527\n",
      " -- Epoch 50  Avg. Training Loss: 0.3498\n"
     ]
    }
   ],
   "source": [
    "training_costs = []\n",
    "for epoch in range(50):\n",
    "    training_loss = []\n",
    "    batch_generator = create_batch_generator(\n",
    "            X_train_centered, y_train, \n",
    "            batch_size=64)\n",
    "    for batch_X, batch_y in batch_generator:\n",
    "        ## prepare a dict to feed data to our network:\n",
    "        feed = {tf_x:batch_X, tf_y:batch_y}\n",
    "        _, batch_cost = sess.run([train_op, cost],\n",
    "                                 feed_dict=feed)\n",
    "        training_costs.append(batch_cost)\n",
    "    print(' -- Epoch %2d  '\n",
    "          'Avg. Training Loss: %.4f' % (\n",
    "              epoch+1, np.mean(training_costs)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 93.89%\n"
     ]
    }
   ],
   "source": [
    "feed = {tf_x : X_test_centered}\n",
    "y_pred = sess.run(predictions['classes'], \n",
    "                  feed_dict=feed)\n",
    " \n",
    "print('Test Accuracy: %.2f%%' % (\n",
    "      100*np.sum(y_pred == y_test)/y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset with Keras API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-3788e55bcadb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmean_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstd_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "mean_vals = np.mean(X_train,axis=0)\n",
    "\n",
    "std_val = np.std(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_centered = (X_train - mean_vals)/std_val\n",
    "X_test_centered = (X_test - mean_vals)/std_val\n",
    " \n",
    "del X_train, X_test\n",
    " \n",
    "print(X_train_centered.shape, y_train.shape)\n",
    "\n",
    "print(X_test_centered.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First 3 labels: ', y_train[:3])\n",
    "print('\\nFirst 3 labels (one-hot):\\n', y_train_onehot[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        units=50,    \n",
    "        input_dim=X_train_centered.shape[1],\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        activation='tanh'))\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        units=50,    \n",
    "        input_dim=50,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        activation='tanh'))\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        units=y_train_onehot.shape[1],    \n",
    "        input_dim=50,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        activation='softmax'))\n",
    "\n",
    "\n",
    "sgd_optimizer = keras.optimizers.SGD(\n",
    "        lr=0.001, decay=1e-7, momentum=.9)\n",
    "\n",
    "model.compile(optimizer=sgd_optimizer,\n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "54000/54000 [==============================] - 3s 47us/step - loss: 0.7247 - val_loss: 0.3621\n",
      "Epoch 2/50\n",
      "54000/54000 [==============================] - 3s 47us/step - loss: 0.3657 - val_loss: 0.2718\n",
      "Epoch 3/50\n",
      "54000/54000 [==============================] - 4s 66us/step - loss: 0.2970 - val_loss: 0.2331\n",
      "Epoch 4/50\n",
      "54000/54000 [==============================] - 3s 60us/step - loss: 0.2604 - val_loss: 0.2105\n",
      "Epoch 5/50\n",
      "54000/54000 [==============================] - 3s 58us/step - loss: 0.2353 - val_loss: 0.1930\n",
      "Epoch 6/50\n",
      "54000/54000 [==============================] - 3s 50us/step - loss: 0.2162 - val_loss: 0.1807\n",
      "Epoch 7/50\n",
      "54000/54000 [==============================] - 3s 56us/step - loss: 0.2006 - val_loss: 0.1718\n",
      "Epoch 8/50\n",
      "54000/54000 [==============================] - 3s 50us/step - loss: 0.1877 - val_loss: 0.1634\n",
      "Epoch 9/50\n",
      "54000/54000 [==============================] - 3s 51us/step - loss: 0.1763 - val_loss: 0.1565\n",
      "Epoch 10/50\n",
      "54000/54000 [==============================] - 3s 52us/step - loss: 0.1664 - val_loss: 0.1504\n",
      "Epoch 11/50\n",
      "54000/54000 [==============================] - 3s 59us/step - loss: 0.1577 - val_loss: 0.1456\n",
      "Epoch 12/50\n",
      "54000/54000 [==============================] - 3s 51us/step - loss: 0.1498 - val_loss: 0.1406\n",
      "Epoch 13/50\n",
      "54000/54000 [==============================] - 3s 61us/step - loss: 0.1424 - val_loss: 0.1377\n",
      "Epoch 14/50\n",
      "54000/54000 [==============================] - 3s 59us/step - loss: 0.1360 - val_loss: 0.1343\n",
      "Epoch 15/50\n",
      "54000/54000 [==============================] - 3s 53us/step - loss: 0.1301 - val_loss: 0.1317\n",
      "Epoch 16/50\n",
      "54000/54000 [==============================] - ETA: 0s - loss: 0.124 - 3s 53us/step - loss: 0.1245 - val_loss: 0.1287\n",
      "Epoch 17/50\n",
      "54000/54000 [==============================] - 3s 52us/step - loss: 0.1194 - val_loss: 0.1264\n",
      "Epoch 18/50\n",
      "54000/54000 [==============================] - 3s 52us/step - loss: 0.1148 - val_loss: 0.1242\n",
      "Epoch 19/50\n",
      "54000/54000 [==============================] - 3s 49us/step - loss: 0.1103 - val_loss: 0.1224\n",
      "Epoch 20/50\n",
      "54000/54000 [==============================] - 3s 52us/step - loss: 0.1062 - val_loss: 0.1211\n",
      "Epoch 21/50\n",
      "54000/54000 [==============================] - 3s 55us/step - loss: 0.1023 - val_loss: 0.1201\n",
      "Epoch 22/50\n",
      "54000/54000 [==============================] - 3s 53us/step - loss: 0.0987 - val_loss: 0.1186\n",
      "Epoch 23/50\n",
      "54000/54000 [==============================] - 3s 50us/step - loss: 0.0952 - val_loss: 0.1180\n",
      "Epoch 24/50\n",
      "54000/54000 [==============================] - 3s 52us/step - loss: 0.0920 - val_loss: 0.1157\n",
      "Epoch 25/50\n",
      "54000/54000 [==============================] - 3s 50us/step - loss: 0.0889 - val_loss: 0.1149\n",
      "Epoch 26/50\n",
      "54000/54000 [==============================] - 3s 49us/step - loss: 0.0858 - val_loss: 0.1147\n",
      "Epoch 27/50\n",
      "54000/54000 [==============================] - 3s 49us/step - loss: 0.0831 - val_loss: 0.1141\n",
      "Epoch 28/50\n",
      "54000/54000 [==============================] - 3s 51us/step - loss: 0.0805 - val_loss: 0.1133\n",
      "Epoch 29/50\n",
      "54000/54000 [==============================] - 3s 49us/step - loss: 0.0779 - val_loss: 0.1121\n",
      "Epoch 30/50\n",
      "54000/54000 [==============================] - 3s 54us/step - loss: 0.0754 - val_loss: 0.1117\n",
      "Epoch 31/50\n",
      "54000/54000 [==============================] - 3s 53us/step - loss: 0.0732 - val_loss: 0.1110\n",
      "Epoch 32/50\n",
      "54000/54000 [==============================] - 3s 48us/step - loss: 0.0709 - val_loss: 0.1114\n",
      "Epoch 33/50\n",
      "54000/54000 [==============================] - 3s 51us/step - loss: 0.0688 - val_loss: 0.1105\n",
      "Epoch 34/50\n",
      "54000/54000 [==============================] - 4s 68us/step - loss: 0.0667 - val_loss: 0.1098\n",
      "Epoch 35/50\n",
      "54000/54000 [==============================] - 3s 52us/step - loss: 0.0647 - val_loss: 0.1095\n",
      "Epoch 36/50\n",
      "54000/54000 [==============================] - 3s 51us/step - loss: 0.0629 - val_loss: 0.1095\n",
      "Epoch 37/50\n",
      "54000/54000 [==============================] - 3s 61us/step - loss: 0.0611 - val_loss: 0.1088\n",
      "Epoch 38/50\n",
      "54000/54000 [==============================] - 3s 52us/step - loss: 0.0594 - val_loss: 0.1087\n",
      "Epoch 39/50\n",
      "54000/54000 [==============================] - 3s 49us/step - loss: 0.0577 - val_loss: 0.1077\n",
      "Epoch 40/50\n",
      "54000/54000 [==============================] - 3s 49us/step - loss: 0.0561 - val_loss: 0.1087\n",
      "Epoch 41/50\n",
      "54000/54000 [==============================] - 3s 51us/step - loss: 0.0546 - val_loss: 0.1079\n",
      "Epoch 42/50\n",
      "54000/54000 [==============================] - 3s 47us/step - loss: 0.0531 - val_loss: 0.1087\n",
      "Epoch 43/50\n",
      "54000/54000 [==============================] - 3s 52us/step - loss: 0.0515 - val_loss: 0.1086\n",
      "Epoch 44/50\n",
      "54000/54000 [==============================] - 3s 59us/step - loss: 0.0503 - val_loss: 0.1074\n",
      "Epoch 45/50\n",
      "54000/54000 [==============================] - 3s 50us/step - loss: 0.0490 - val_loss: 0.1076\n",
      "Epoch 46/50\n",
      "54000/54000 [==============================] - 3s 48us/step - loss: 0.0477 - val_loss: 0.1071\n",
      "Epoch 47/50\n",
      "54000/54000 [==============================] - 3s 51us/step - loss: 0.0464 - val_loss: 0.1077\n",
      "Epoch 48/50\n",
      "54000/54000 [==============================] - 3s 49us/step - loss: 0.0453 - val_loss: 0.1083\n",
      "Epoch 49/50\n",
      "54000/54000 [==============================] - 3s 50us/step - loss: 0.0440 - val_loss: 0.1077\n",
      "Epoch 50/50\n",
      "54000/54000 [==============================] - 3s 50us/step - loss: 0.0430 - val_loss: 0.1078\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_centered, y_train_onehot,\n",
    "                    batch_size=64, epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = model.predict_classes(X_train_centered,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_train_predictions = np.sum(y_train==y_train_preds,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = correct_train_predictions/y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.97833333333334"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predictions= model.predict_classes(X_test_centered,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_test_preds = np.sum(y_test==y_test_predictions,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_accuracy = correct_test_preds/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.1"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
